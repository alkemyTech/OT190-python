from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python import PythonOperator
from plugin.university_group_G import DataProcessor


function_dag = DataProcessor("{{dag_id}}")


default_args = {
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}


with DAG(dag_id='ETL_Universidad_J_F_Kennedy',
    description='DAG Universidad_J_F_Kennedy',
    start_date = datetime(2022, 5, 9),  
    schedule_interval= timedelta(hours=1),
    catchup=False ) as dag:

        extract_data = PythonOperator(task_id='extract_data',
                                  python_callable= function_dag.extract_data_sql)

        process_data = PythonOperator(task_id='process_data', 
                                python_callable= function_dag.transform_data)


        load_data = LocalFilesystemToS3Operator(
            task_id='load_data',
            filename=f'{parent_path}/datasets/{file_university}',
            dest_key='aws_s3_alkemy_universidades',
            dest_bucket='cohorte-abril-98a56bb4',
            replace=True,
        )

        extract_data >> process_data >> load_data